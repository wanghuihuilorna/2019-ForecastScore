{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "path = '/home/wjunneng/Python/ForecastScore/'\n",
    "\n",
    "submission_s1 = '/data/test_s1/submission_s1.csv'\n",
    "submission_s1_sample_mean_median = '/data/test_s1/submission_s1_sample_mean_median.csv'\n",
    "submission_s1_sample_svm = '/data/test_s1/submission_s1_sample_svm.csv'\n",
    "submission_s1_sample_xgb = '/data/test_s1/submission_s1_sample_xgb.csv'\n",
    "submission_s1_sample_lgb = '/data/test_s1/submission_s1_sample_lgb.csv'\n",
    "\n",
    "submission_s1_sample_mean_median1 = '/data/test_s1/submission_s1_sample_mean_median1.csv'\n",
    "\n",
    "course = '/data/train_s1/course.csv'\n",
    "student = '/data/train_s1/student.csv'\n",
    "exam_score = '/data/train_s1/exam_score.csv'\n",
    "all_knowledge = '/data/train_s1/all_knowledge.csv'\n",
    "course1_exams = '/data/train_s1/course1_exams.csv'\n",
    "course2_exams = '/data/train_s1/course2_exams.csv'\n",
    "course3_exams = '/data/train_s1/course3_exams.csv'\n",
    "course4_exams = '/data/train_s1/course3_exams.csv'\n",
    "course5_exams = '/data/train_s1/course5_exams.csv'\n",
    "course6_exams = '/data/train_s1/course6_exams.csv'\n",
    "course7_exams = '/data/train_s1/course7_exams.csv'\n",
    "course8_exams = '/data/train_s1/course8_exams.csv'\n",
    "\n",
    "submission_s1_sample = '/data/submission_s1_sample.csv'\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_train_s1(file_name, tag):\n",
    "    \"\"\"\n",
    "    返回train_s1文件夹下的文件\n",
    "    :param file_name: 文件名称\n",
    "    :param tag: 返回的数据类型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tag == 'np':\n",
    "        return np.loadtxt(path + file_name, delimiter=',', dtype=np.str)\n",
    "    elif tag == 'pd':\n",
    "        return pd.read_csv(path + file_name)\n",
    "    else:\n",
    "        return \"请检查文件名/需要返回的数据类型\"\n",
    "\n",
    "\n",
    "def get_test_s1(file_name, tag):\n",
    "    \"\"\"\n",
    "    返回test_s1文件夹下的文件\n",
    "    :param file_name: 文件名称\n",
    "    :param tag: 返回的数据类型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tag == 'np':\n",
    "        return np.loadtxt(path + file_name, delimiter=',', dtype=np.str)\n",
    "    elif tag == 'pd':\n",
    "        return pd.read_csv(path + file_name)\n",
    "    else:\n",
    "        return \"请检查文件名/需要返回的数据类型\"\n",
    "\n",
    "\n",
    "def get_sample(file_name, tag):\n",
    "    \"\"\"\n",
    "    返回范例文件夹下的文件\n",
    "    :param file_name: 文件名称\n",
    "    :param tag: 返回的数据类型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tag == 'np':\n",
    "        return np.loadtxt(path + file_name, delimiter=',', dtype=np.str)\n",
    "    elif tag == 'pd':\n",
    "        return pd.read_csv(path + file_name)\n",
    "    else:\n",
    "        return \"请检查文件名/需要返回的数据类型\"\n",
    "\n",
    "\n",
    "# %%\n",
    "exam_score_data = get_train_s1(exam_score, 'pd')\n",
    "\n",
    "reuslt = []\n",
    "for i in ['course1', 'course2', 'course3', 'course4', 'course5', 'course6', 'course7', 'course8']:\n",
    "    exam_score_course = exam_score_data[exam_score_data.course == i]\n",
    "    for exam_score_course_student in exam_score_course.groupby('student_id'):\n",
    "        print('--' + str(exam_score_course_student[1].shape) + '--')\n",
    "        tmp = exam_score_course_student[1].score\n",
    "        if tmp.values.min() == 0:\n",
    "            print(exam_score_course_student[1].shape)\n",
    "\n",
    "print(exam_score_data.describe())\n",
    "print(exam_score_data.columns)\n",
    "\n",
    "\n",
    "# %%\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    减少内存消耗\n",
    "    :param df:\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "            start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# %%\n",
    "# 运行速度慢  暂时丢弃\n",
    "import time\n",
    "\n",
    "reuslt = []\n",
    "\n",
    "start = time.clock()\n",
    "for i in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "    print(time.clock() - start)\n",
    "\n",
    "    # submission_path = path + '/data/cache/submission_s1_course' + i + '.h5'\n",
    "    # test_X = reduce_mem_usage(pd.read_hdf(path_or_buf=submission_path, mode='r', key='course'+i))\n",
    "    # del test_X['pred']\n",
    "\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + i + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + i))\n",
    "\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "    # LGBMRegressor\n",
    "    model = lgb.LGBMRegressor(boosting_type='dart', random_state=50,\n",
    "                              objective='regression', subsample=0.6143)\n",
    "\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    params_spaces = {\n",
    "        'learning_rate': (0.01, 0.5),\n",
    "        'num_leaves': (10, 100),\n",
    "        'max_depth': (30, 100),\n",
    "        'min_child_samples': (0, 50),\n",
    "        'max_bin': (100, 1000),\n",
    "        'subsample_freq': (0, 10),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'reg_lambda': (1e-9, 100),\n",
    "        'reg_alpha': (1e-9, 1.0),\n",
    "        'scale_pos_weight': (1e-6, 100),\n",
    "        'n_estimators': (100, 400),\n",
    "    }\n",
    "\n",
    "    gs_cv_tuner = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=params_spaces,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=folds,\n",
    "        verbose=0,\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "        n_jobs=8)\n",
    "\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    train_X = train_X[train_X.exam_id.isin(exam_id[:int(len(exam_id) - 2)])]\n",
    "    # 获取测试集\n",
    "    test_X = train_X[train_X.exam_id.isin(exam_id[int(len(exam_id) - 2):])]\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    test_y = test_X['score']\n",
    "\n",
    "    del train_X['score']\n",
    "    del test_X['score']\n",
    "\n",
    "    result = gs_cv_tuner.fit(train_X, train_y)\n",
    "\n",
    "    print(\"val. score: %s\" % result.best_score_)\n",
    "    print(\"test score: %s\" % result.score(test_X, test_y))\n",
    "# %%\n",
    "#######################################  使用验证集验证模型xgb  #####################################################\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "models = []\n",
    "\n",
    "start = time.clock()\n",
    "for i in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + i + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + i))\n",
    "\n",
    "    #######################################  训练集-2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    train_X = train_X[train_X.exam_id.isin(exam_id[:int(len(exam_id) - 2)])]\n",
    "    #######################################  训练集-2  ############################################################\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model = XGBRegressor(boosting_type='dart')\n",
    "    search_params = {\n",
    "        'max_depth': list(range(1, 10, 1)),\n",
    "        'n_estimators': list(range(10, 1000, 10)),\n",
    "        'gamma': list(np.arange(0., 1, .05)),\n",
    "        'min_child_weight': list(np.arange(0.1, 1, .1)),\n",
    "        'colsample_bytree': list(np.arange(0.1, 1, .05)),\n",
    "        'learning_rate': list(np.arange(0.01, 0.1, 0.01))\n",
    "    }\n",
    "    search = RandomizedSearchCV(model, search_params, n_iter=50, n_jobs=8)\n",
    "\n",
    "    search.fit(train_X, train_y)\n",
    "    models.append(search.best_estimator_)\n",
    "    print(search.best_estimator_)\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    index += 1\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + str(index) + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + str(index)))\n",
    "\n",
    "    #######################################  训练集-2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    train_X = train_X[train_X.exam_id.isin(exam_id[:int(len(exam_id) - 2)])]\n",
    "    #######################################  训练集-2  ############################################################\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    with open(path + '/model/xgb_model_' + str(index) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print('成功保存xgb_model_' + str(index) + '.pkl!')\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for index, model_file in enumerate(['xgb_model_1.pkl', 'xgb_model_2.pkl', 'xgb_model_3.pkl',\n",
    "                                    'xgb_model_4.pkl', 'xgb_model_5.pkl', 'xgb_model_6.pkl',\n",
    "                                    'xgb_model_7.pkl', 'xgb_model_8.pkl']):\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    index += 1\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + str(index) + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + str(index)))\n",
    "\n",
    "    #######################################  2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    test_X = train_X[train_X.exam_id.isin(exam_id[int(len(exam_id) - 2):])]\n",
    "    #######################################  2  ############################################################\n",
    "\n",
    "    test_y = test_X['score']\n",
    "    del test_X['score']\n",
    "\n",
    "    with open(path + '/model/' + model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "        predictions = model.predict(test_X)\n",
    "        test_y = np.array(test_y)\n",
    "        predictions = np.array(predictions)\n",
    "        rmse = np.sqrt(((test_y - predictions) ** 2).mean())\n",
    "\n",
    "        print('rmse：' + str(rmse))\n",
    "\n",
    "# %%\n",
    "#######################################  使用验证集验证模型lgb  #####################################################\n",
    "import pickle\n",
    "import time\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "models = []\n",
    "\n",
    "start = time.clock()\n",
    "for i in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + i + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + i))\n",
    "\n",
    "    #######################################  训练集-2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    train_X = train_X[train_X.exam_id.isin(exam_id[:int(len(exam_id) - 2)])]\n",
    "    #######################################  训练集-2  ############################################################\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model = LGBMRegressor(boosting_type='dart', objective='regression', n_estimators=1000, max_depth=10,\n",
    "                          learning_rate=0.1)\n",
    "    search_params = {\n",
    "        'max_depth': list(range(1, 10, 1)),\n",
    "        'n_estimators': list(range(10, 1000, 10)),\n",
    "        'gamma': list(np.arange(0., 1, .05)),\n",
    "        'min_child_weight': list(np.arange(0.1, 1, .1)),\n",
    "        'colsample_bytree': list(np.arange(0.1, 1, .05)),\n",
    "        'learning_rate': list(np.arange(0.01, 0.1, 0.01))\n",
    "    }\n",
    "    search = RandomizedSearchCV(model, search_params, n_iter=100, n_jobs=8)\n",
    "\n",
    "    search.fit(train_X, train_y)\n",
    "    models.append(search.best_estimator_)\n",
    "    print(search.best_estimator_)\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    index += 1\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + str(index) + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + str(index)))\n",
    "\n",
    "    #######################################  训练集-2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    train_X = train_X[train_X.exam_id.isin(exam_id[:int(len(exam_id) - 2)])]\n",
    "    #######################################  训练集-2  ############################################################\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    with open(path + '/model/lgb_model_' + str(index) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print('成功保存lgb_model_' + str(index) + '.pkl!')\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for index, model_file in enumerate(['lgb_model_1.pkl', 'lgb_model_2.pkl', 'lgb_model_3.pkl',\n",
    "                                    'lgb_model_4.pkl', 'lgb_model_5.pkl', 'lgb_model_6.pkl',\n",
    "                                    'lgb_model_7.pkl', 'lgb_model_8.pkl']):\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    index += 1\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + str(index) + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + str(index)))\n",
    "\n",
    "    #######################################  2  ############################################################\n",
    "    # 获取exam_id\n",
    "    exam_id = list(set(train_X.exam_id))\n",
    "    # 对exam_id进行排序\n",
    "    exam_id.sort()\n",
    "    # 获取训练集\n",
    "    test_X = train_X[train_X.exam_id.isin(exam_id[int(len(exam_id) - 2):])]\n",
    "    #######################################  2  ############################################################\n",
    "\n",
    "    test_y = test_X['score']\n",
    "    del test_X['score']\n",
    "\n",
    "    with open(path + '/model/' + model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "        predictions = model.predict(test_X)\n",
    "        test_y = np.array(test_y)\n",
    "        predictions = np.array(predictions)\n",
    "        rmse = np.sqrt(((test_y - predictions) ** 2).mean())\n",
    "\n",
    "        print('rmse：' + str(rmse))\n",
    "# %%\n",
    "#######################################  使用训练集训练模型xgb  #####################################################\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "models = []\n",
    "\n",
    "start = time.clock()\n",
    "for i in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + i + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + i))\n",
    "\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model = XGBRegressor(boosting_type='dart')\n",
    "    search_params = {\n",
    "        'max_depth': list(range(1, 10, 1)),\n",
    "        'n_estimators': list(range(10, 1000, 10)),\n",
    "        'gamma': list(np.arange(0., 1, .05)),\n",
    "        'min_child_weight': list(np.arange(0.1, 1, .1)),\n",
    "        'colsample_bytree': list(np.arange(0.1, 1, .05)),\n",
    "        'learning_rate': list(np.arange(0.01, 0.1, 0.01))\n",
    "    }\n",
    "    search = RandomizedSearchCV(model, search_params, n_iter=50, n_jobs=8)\n",
    "\n",
    "    search.fit(train_X, train_y)\n",
    "    models.append(search.best_estimator_)\n",
    "    print(search.best_estimator_)\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    print('耗时：%s \\n' % str(time.clock() - start))\n",
    "\n",
    "    index += 1\n",
    "    exam_score_path = path + '/data/cache/exam_score_course' + str(index) + '.h5'\n",
    "    train_X = reduce_mem_usage(pd.read_hdf(path_or_buf=exam_score_path, mode='r', key='course' + str(index)))\n",
    "    train_y = train_X['score']\n",
    "    del train_X['score']\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    with open(path + '/model/xgb_model_' + str(index) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print('成功保存xgb_model_' + str(index) + '.pkl!')\n",
    "# %%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-e50da876",
   "language": "python",
   "display_name": "PyCharm (ForecastScore)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}